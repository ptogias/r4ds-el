{
  "hash": "631751d0fbb11e2ddaea252d7a3fc70f",
  "result": {
    "markdown": "---\nfreeze: true\n---\n\n\n# Arrow {#sec-arrow}\n\n\n::: {.cell}\n\n:::\n\n\n## Εισαγωγή\n\nΤα αρχεία CSV έχουν σχεδιαστεί για να διαβάζονται εύκολα από τον άνθρωπο.\nΕίναι μία καλή μορφή για ανταλλαγή πληροφορίας επειδή είναι πολύ απλά και μπορούν να διαβαστούν από κάθε εργαλείο.\nΤα αρχεία CSV όμως δεν είναι πολύ αποτελεσματικά: πρέπει να κάνετε αρκετή δουλειά για να διαβάσετε τα δεδομένα στην R.\nΣε αυτό το κεφάλαιο, θα μάθετε για μία ισχυρή εναλλακτική: τη [μορφή parquet](https://parquet.apache.org/), μία μορφή που βασίζεται σε ανοιχτά πρότυπα και χρησιμοποιείται ευρέως από συστήματα διαχείρισης μεγάλων δεδομένων.\n\nΘα συνδυάσουμε αρχεία parquet με το [Apache Arrow](https://arrow.apache.org), μία εργαλειοθήκη πολλαπλών γλωσσών, σχεδιασμένη για αποτελεσματική ανάλυση και μεταφορά μεγάλων συνόλων δεδομένων.\nΘα χρησιμοποιήσουμε το Apache Arrow μέσω του [πακέτου arrow](https://arrow.apache.org/docs/r/), το οποίο παρέχει ένα σύστημα υποστήριξης της dplyr που σας επιτρέπει να αναλύετε σύνολα δεδομένων μεγαλύτερα από τη μνήμη, χρησιμοποιώντας την γνωστή σύνταξη από το πακέτο dplyr.\nΩς πρόσθετο πλεονέκτημα, το arrow είναι εξαιρετικά γρήγορο: θα δείτε μερικά παραδείγματα αργότερα στο κεφάλαιο.\n\nΤόσο το arrow όσο και το πακέτο dbplyr παρέχουν συστήματα υποστήριξης στο πακέτο dplyr, οπότε μπορεί να αναρωτιέστε πότε να χρησιμοποιήσετε το καθένα.\nΣε πολλές περιπτώσεις, αυτή η επιλογή έχει ήδη γίνει για εσάς, καθώς τα δεδομένα βρίσκονται ήδη σε μία βάση δεδομένων ή σε αρχεία parquet, οπότε θα θέλετε να εργαστείτε με αυτά ως έχουν.\nΕάν όμως ξεκινάτε με τα δικά σας δεδομένα (ίσως αρχεία CSV), μπορείτε είτε να τα φορτώσετε σε μία βάση δεδομένων είτε να τα μετατρέψετε σε parquet.\nΓενικά, είναι δύσκολο να γνωρίζουμε τι θα λειτουργήσει καλύτερα, επομένως στα πρώτα στάδια της ανάλυσής σας, θα σας ενθαρρύνουμε να δοκιμάσετε και τα δύο και να επιλέξετε αυτό που λειτουργεί καλύτερα για εσάς.\n\n(Ένα μεγάλο ευχαριστώ στη Danielle Navarro που συνέβαλε στην αρχική έκδοση αυτού του κεφαλαίου.)\n\n### Προαπαιτούμενα\n\nΣε αυτό το κεφάλαιο, θα συνεχίσουμε να χρησιμοποιούμε το tidyverse, και ιδιαίτερα το πακέτο dplyr, αλλά θα το συνδυάσουμε με το πακέτο arrow που έχει σχεδιαστεί ειδικά για εργασία με μεγάλα δεδομένα.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(arrow)\n```\n:::\n\n\nΑργότερα στο κεφάλαιο, θα δούμε επίσης κάποιες συνδέσεις μεταξύ arrow και duckdb, κι επομένως θα χρειαστούμε επίσης το πακέτο dbplyr και το duckdb.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dbplyr, warn.conflicts = FALSE)\nlibrary(duckdb)\n#> Loading required package: DBI\n```\n:::\n\n\n## Λήψη δεδομένων\n\nΞεκινάμε παίρνοντας ένα σύνολο δεδομένων αντάξιο αυτών των εργαλείων: ένα σύνολο δεδομένων από βιβλία που έχουν δανειστεί από τις δημόσιες βιβλιοθήκες του Σιάτλ.\nΕίναι διαθέσιμο στο διαδίκτυο στη διεύθυνση [data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6](https://data.%20seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6).\nΑυτό το σύνολο δεδομένων περιέχει 41.389.465 γραμμές που σας λένε πόσες φορές κάθε βιβλίο έχει δανειστεί κάθε μήνα από τον Απρίλιο του 2005 έως τον Οκτώβριο του 2022.\n\nΟ παρακάτω κώδικας θα σας δώσει ένα αποθηκευμένο αντίγραφο των δεδομένων.\nΤα δεδομένα είναι ένα αρχείο CSV 9 GB, επομένως θα χρειαστεί λίγος χρόνος για τη λήψη του.\nΣυνιστώ ανεπιφύλακτα να χρησιμοποιήσετε το `curl::multi_download()` για να κατεβάζετε πολύ μεγάλα αρχεία, καθώς έχει κατασκευαστεί γι' αυτόν ακριβώς τον σκοπό: σας δίνει μία γραμμή προόδου και μπορεί να συνεχίσει τη λήψη εάν διακοπεί.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndir.create(\"data\", showWarnings = FALSE)\n\ncurl::multi_download(\n  \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  \"data/seattle-library-checkouts.csv\",\n  resume = TRUE\n)\n```\n:::\n\n\n## Άνοιγμα ενός συνόλου δεδομένων\n\nΑς ξεκινήσουμε ρίχνοντας μία ματιά στα δεδομένα.\nΜε μέγεθος 9 GB, αυτό το αρχείο είναι αρκετά μεγάλο, οπότε πιθανώς να μην θέλουμε να το φορτώσουμε ολόκληρο στη μνήμη.\nΈνας καλός εμπειρικός κανόνας είναι ότι συνήθως θέλετε να έχετε τουλάχιστον διπλάσια μνήμη από το μέγεθος των δεδομένων, και πολλοί φορητοί υπολογιστές φτάνουν μέχρι τα 16 GB.\nΑυτό σημαίνει ότι θέλουμε να αποφύγουμε την `read_csv()` και προτιμάμε να χρησιμοποιήσουμε την `arrow::open_dataset()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv <- open_dataset(\n  sources = \"data/seattle-library-checkouts.csv\", \n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)\n```\n:::\n\n\nΤι συμβαίνει όταν εκτελείται αυτός ο κώδικας;\\\nΗ `open_dataset()` θα σκανάρει μερικές χιλιάδες γραμμές για να καταλάβει τη δομή του συνόλου δεδομένων.\nΗ στήλη `ISBN` περιέχει κενές τιμές για τις πρώτες 80.000 σειρές, επομένως πρέπει να καθορίσουμε τον τύπο στήλης για να βοηθήσουμε το arrow να επεξεργαστεί τη δομή των δεδομένων.\nΜόλις η `open_dataset()` σαρώσει τα δεδομένα, καταγράφει αυτό που βρέθηκε και σταματά.\nΘα διαβάζει περαιτέρω γραμμές μόνο όταν τις ζητάτε.\nΑυτά είναι τα μεταδεδομένα που βλέπουμε αν εκτυπώσουμε το `seattle_csv`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv\n#> FileSystemDataset with 1 csv file\n#> UsageClass: string\n#> CheckoutType: string\n#> MaterialType: string\n#> CheckoutYear: int64\n#> CheckoutMonth: int64\n#> Checkouts: int64\n#> Title: string\n#> ISBN: string\n#> Creator: string\n#> Subjects: string\n#> Publisher: string\n#> PublicationYear: string\n```\n:::\n\n\nΗ πρώτη γραμμή στην έξοδο σας λέει ότι το `seattle_csv` αποθηκεύεται τοπικά στο δίσκο ως ένα μεμονωμένο αρχείο CSV.\nθα φορτωθεί στη μνήμη μόνο όταν απαιτείται.\nΤο υπόλοιπο της εξόδου σας λέει τον τύπο στήλης που έχει υπολογίσει το arrow για κάθε στήλη.\n\nΜπορούμε να δούμε τι συμβαίνει στην πραγματικότητα με την `glimpse()`.\nΑυτό αποκαλύπτει ότι υπάρχουν \\~41 εκατομμύρια γραμμές και 12 στήλες και, ακόμα, μας εμφανίζει και μερικές τιμές.\n\n\n::: {.cell hash='arrow_cache/html/glimpse-data_07c924738790eb185ebdd8973443e90d'}\n\n```{.r .cell-code}\nseattle_csv |> glimpse()\n#> FileSystemDataset with 1 csv file\n#> 41,389,465 rows x 12 columns\n#> $ UsageClass      <string> \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Ph…\n#> $ CheckoutType    <string> \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Hor…\n#> $ MaterialType    <string> \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOO…\n#> $ CheckoutYear     <int64> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 20…\n#> $ CheckoutMonth    <int64> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n#> $ Checkouts        <int64> 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2,…\n#> $ Title           <string> \"Super rich : a guide to having it all / Russell S…\n#> $ ISBN            <string> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n#> $ Creator         <string> \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim …\n#> $ Subjects        <string> \"Self realization, Conduct of life, Attitude Psych…\n#> $ Publisher       <string> \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Di…\n#> $ PublicationYear <string> \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c20…\n```\n:::\n\n\nΜπορούμε να αρχίσουμε να χρησιμοποιούμε αυτό το σύνολο δεδομένων με συναρτήσεις του πακέτου dplyr, χρησιμοποιώντας την `collect()` για να αναγκάσουμε το arrow να εκτελέσει τον υπολογισμό και να επιστρέψει κάποια δεδομένα.\nΓια παράδειγμα, αυτός ο κωδικός μας λέει τον συνολικό αριθμό δανεισμών ανά έτος:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> \n  group_by(CheckoutYear) |> \n  summarise(Checkouts = sum(Checkouts)) |> \n  arrange(CheckoutYear) |> \n  collect()\n#> # A tibble: 18 × 2\n#>   CheckoutYear Checkouts\n#>          <int>     <int>\n#> 1         2005   3798685\n#> 2         2006   6599318\n#> 3         2007   7126627\n#> 4         2008   8438486\n#> 5         2009   9135167\n#> 6         2010   8608966\n#> # ℹ 12 more rows\n```\n:::\n\n\nΧάρη στο arrow, αυτός ο κώδικας θα λειτουργεί ανεξάρτητα από το πόσο μεγάλο είναι το υποκείμενο σύνολο δεδομένων.\nΑλλά αυτή τη στιγμή είναι αρκετά αργός: στον υπολογιστή του Hadley, χρειάστηκαν \\~10 δευτερόλεπτα για να τρέξει.\nΑυτό δεν είναι τρομερό δεδομένου του όγκου των δεδομένων που έχουμε, μπορούμε όμως να τα κάνουμε πολύ πιο γρήγορα μεταβαίνοντας σε μία καλύτερη μορφή.\n\n## Η μορφή parquet {#sec-parquet}\n\nΓια να διευκολύνουμε την εργασία με αυτά τα δεδομένα, ας αλλάξουμε στη μορφή αρχείου parquet και ας τα χωρίσουμε σε πολλαπλά αρχεία.\nΟι παρακάτω ενότητες θα σας παρουσιάσουν πρώτα το parquet και την διαμέριση και, στη συνέχεια, θα εφαρμόσουν όσα μάθαμε στα δεδομένα της βιβλιοθήκης του Σιάτλ.\n\n### Πλεονεκτήματα της parquet\n\nΌπως το CSV, έτσι και το parquet χρησιμοποιείται για δεδομένα σε μορφή πίνακα, αλλά αντί τα δεδομένα μας να είναι σε μία μορφή κειμένου που μπορείτε να διαβάσετε με οποιοδήποτε πρόγραμμα επεξεργασίας αρχείων, είναι μία προσαρμοσμένη δυαδική μορφή που έχει σχεδιαστεί ειδικά για τις ανάγκες των μεγάλων δεδομένων.\nΑυτό σημαίνει ότι:\n\n-   Τα αρχεία parquet είναι συνήθως μικρότερα από το αντίστοιχο αρχείο CSV.\n    Το parquet βασίζεται σε [αποδοτικές κωδικοποιήσεις](https://parquet.apache.org/docs/file-format/data-pages/encodings/) για να διατηρεί το μέγεθος του αρχείου χαμηλό και να υποστηρίζει τη συμπίεση αρχείων.\n    Αυτό βοηθά εργασίες που αφορούν αρχεία parquet να γίνουν πιο γρήγορες επειδή υπάρχουν λιγότερα δεδομένα για μετακίνηση από τον δίσκο προς τη μνήμη.\n\n-   Τα αρχεία parquet έχουν ένα σύστημα που επιτρέπει ευέλικτους τρόπους περιγραφής των τύπων δεδομένων.\n    Όπως είπαμε στην @sec-col-types, ένα αρχείο CSV δεν παρέχει πληροφορίες σχετικά με τους τύπους των στηλών του.\n    Για παράδειγμα, ένα σύστημα αναγνώστης CSV πρέπει να μαντέψει εάν το \"08-10-2022\" πρέπει να αναλυθεί ως συμβολοσειρά ή ως ημερομηνία.\n    Αντίθετα, τα αρχεία parquet αποθηκεύουν τα δεδομένα με τρόπο που καταγράφει και τον τύπο τους.\n\n-   Τα αρχεία parquet είναι \"προσαρμοσμένα σε στήλες\".\n    Αυτό σημαίνει ότι είναι οργανωμένα στήλη προς στήλη, όπως το πλαίσιο δεδομένων της R.\n    Αυτό συνήθως οδηγεί σε καλύτερη απόδοση για εργασίες ανάλυσης δεδομένων σε σύγκριση με αρχεία CSV, τα οποία είναι οργανωμένα σε γραμμές.\n\n-   Τα αρχεία parquet είναι \"σε κομμάτια\", γεγονός που καθιστά δυνατή την εργασία σε διαφορετικά μέρη του αρχείου ταυτόχρονα και, αν είστε τυχεροί, μπορείτε να παραλείψετε μερικά κομμάτια εντελώς.\n\nΥπάρχει ένα βασικό μειονέκτημα στα αρχεία parquet: δεν είναι πλέον \"αναγνώσιμα από τον άνθρωπο\", δηλαδή εάν κοιτάξετε ένα αρχείο parquet χρησιμοποιώντας την `readr::read_file()`, θα δείτε απλώς μία δέσμη ασυναρτησιών.\n\n### Διαμερισμός\n\nΚαθώς τα σύνολα δεδομένων γίνονται όλο και μεγαλύτερα, η αποθήκευση όλων των δεδομένων σε ένα μόνο αρχείο γίνεται όλο και πιο επίπονη, και είναι συχνά χρήσιμο μεγάλα σύνολα δεδομένων να χωρίζονται σε πολλαπλά αρχεία.\nΌταν αυτή η δόμηση γίνει έξυπνα, η στρατηγική αυτή μπορεί να οδηγήσει σε σημαντικές βελτιώσεις στην απόδοση, επειδή πολλές αναλύσεις θα απαιτούν μόνο ένα υποσύνολο των αρχείων.\n\nΔεν υπάρχουν συγκεκριμένοι και γρήγοροι κανόνες σχετικά με τον τρόπο διαχωρισμού των δεδομένων σας: τα αποτελέσματα θα εξαρτηθούν από τα δεδομένα σας, τα μοτίβα πρόσβασης και τα συστήματα που διαβάζουν τα δεδομένα.\nΕίναι πιθανό να χρειαστεί να κάνετε κάποιους πειραματισμούς προτού βρείτε τον ιδανικό διαχωρισμό για την περίπτωσή σας.\nΩς γενικό οδηγό, το arrow προτείνει να αποφεύγετε αρχεία μικρότερα από 20 MB και μεγαλύτερα από 2 GB και να αποφεύγετε διαμερίσματα που παράγουν περισσότερα από 10.000 αρχεία.\nΘα πρέπει επίσης να προσπαθήσετε να χωρίσετε τα αρχεία σύμφωνα με τις μεταβλητές που φιλτράρετε.\nΌπως θα δείτε σύντομα, αυτό επιτρέπει στο arrow να παραλείψει αρκετούς υπολογισμούς διαβάζοντας μόνο τα σχετικά αρχεία.\n\n### Ξαναγράφοντας τα δεδομένα της βιβλιοθήκης του Σιάτλ\n\nΑς εφαρμόσουμε αυτές τις ιδέες στα δεδομένα της βιβλιοθήκης του Σιάτλ για να δούμε πως λειτουργούν στην πράξη.\nΘα κάνουμε τον διαχωρισμό με βάση τη στήλη `CheckoutYear`, καθώς είναι πιθανό ορισμένες αναλύσεις να θέλουν να εξετάσουν μόνο τα πρόσφατα δεδομένα και ο διαχωρισμός ανά έτος επιστρέφει 18 κομμάτια λογικού μεγέθους.\n\nΓια να ξαναγράψουμε τα δεδομένα ορίζουμε το διαμέρισμα χρησιμοποιώντας την `dplyr::group_by()` και, στη συνέχεια, αποθηκεύουμε τα διαμερίσματα σε έναν κατάλογο με την `arrow::write_dataset()`.\nΗ `write_dataset()` έχει δύο σημαντικά ορίσματα: έναν κατάλογο στον οποίο θα δημιουργήσουμε τα αρχεία και τη μορφή που θα χρησιμοποιήσουμε.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npq_path <- \"data/seattle-library-checkouts\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  write_dataset(path = pq_path, format = \"parquet\")\n```\n:::\n\n\nΑυτό παίρνει περίπου ένα λεπτό για να τρέξει.\nΌπως θα δούμε σύντομα αυτή είναι μία αρχική επένδυση που αποδίδει καρπούς κάνοντας μελλοντικές εργασίες πολύ πολύ πιο γρήγορες.\n\nΑς ρίξουμε μία ματιά σε αυτό που μόλις δημιουργήσαμε:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  files = list.files(pq_path, recursive = TRUE),\n  size_MB = file.size(file.path(pq_path, files)) / 1024^2\n)\n#> # A tibble: 18 × 2\n#>   files                            size_MB\n#>   <chr>                              <dbl>\n#> 1 CheckoutYear=2005/part-0.parquet    109.\n#> 2 CheckoutYear=2006/part-0.parquet    164.\n#> 3 CheckoutYear=2007/part-0.parquet    178.\n#> 4 CheckoutYear=2008/part-0.parquet    195.\n#> 5 CheckoutYear=2009/part-0.parquet    214.\n#> 6 CheckoutYear=2010/part-0.parquet    222.\n#> # ℹ 12 more rows\n```\n:::\n\n\nΤο αρχείο CSV μας με τα 9 GB έχει ξαναγραφτεί σε 18 αρχεία parquet.\nΤα ονόματα αρχείων χρησιμοποιούν έναν κανόνα \"αυτο-περιγραφής\" που χρησιμοποιείται από το [Apache Hive](https://hive.apache.org).\nΔιαμερίσματα τύπου Hive ονομάζουν φακέλους με έναν κανόνα \"key=value\", έτσι, όπως μπορείτε να μαντέψετε, ο κατάλογος `CheckoutYear=2005` περιέχει όλα τα δεδομένα όπου η `CheckoutYear` είναι 2005.\nΚάθε αρχείο είναι μεταξύ 100 και 300 MB και το συνολικό μέγεθος είναι τώρα περίπου 4 GB, λίγο περισσότερο από το μισό μέγεθος του αρχικού αρχείου CSV.\nΑναμενόμενο, καθώς το parquet είναι μία πολύ πιο αποτελεσματική μορφή αρχείου.\n\n## Συνδυάζοντας τα πακέτα dplyr και arrow\n\nΑφού δημιουργήσαμε αυτά τα αρχεία parquet, τώρα θα πρέπει να τα διαβάσουμε ξανά.\nΧρησιμοποιούμε ξανά την `open_dataset()`, αλλά αυτή τη φορά της δίνουμε έναν κατάλογο:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq <- open_dataset(pq_path)\n```\n:::\n\n\nΤώρα μπορούμε να γράψουμε τη ροή με το πακέτο dplyr.\nΓια παράδειγμα, θα μπορούσαμε να μετρήσουμε τον συνολικό αριθμό βιβλίων που είχαν δανειστεί κάθε μήνα για τα τελευταία πέντε χρόνια:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery <- seattle_pq |> \n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear, CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(CheckoutYear, CheckoutMonth)\n```\n:::\n\n\nΗ σύνταξη κώδικα dplyr για δεδομένα arrow είναι εννοιολογικά παρόμοια με αυτή του πακέτου dbplyr, @sec-import-databases: γράφετε κώδικα dplyr, ο οποίος μετατρέπεται αυτόματα σε ένα ερώτημα που κατανοεί η βιβλιοθήκη της Apache Arrow C++, και το οποίο ερώτημα στη συνέχεια εκτελείται όταν καλείτε την `collect()`.\nΕάν εκτυπώσουμε το αντικείμενο `query`, μπορούμε να δούμε μερικές πληροφορίες σχετικά με το τι περιμένουμε να επιστρέψει το Arrow όταν πραγματοποιηθεί η εκτέλεση:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery\n#> FileSystemDataset (query)\n#> CheckoutYear: int32\n#> CheckoutMonth: int64\n#> TotalCheckouts: int64\n#> \n#> * Grouped by CheckoutYear\n#> * Sorted by CheckoutYear [asc], CheckoutMonth [asc]\n#> See $.data for the source Arrow object\n```\n:::\n\n\nΚαι μπορούμε να πάρουμε τα αποτελέσματα καλώντας την `collect()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquery |> collect()\n#> # A tibble: 58 × 3\n#> # Groups:   CheckoutYear [5]\n#>   CheckoutYear CheckoutMonth TotalCheckouts\n#>          <int>         <int>          <int>\n#> 1         2018             1         355101\n#> 2         2018             2         309813\n#> 3         2018             3         344487\n#> 4         2018             4         330988\n#> 5         2018             5         318049\n#> 6         2018             6         341825\n#> # ℹ 52 more rows\n```\n:::\n\n\nΌπως και το dbplyr, έτσι και το arrow κατανοεί μόνο ορισμένες εκφράσεις της R, επομένως ενδέχεται να μην μπορείτε να γράψετε ακριβώς τον ίδιο κώδικα που θα γράφατε συνήθως.\nΩστόσο, η λίστα των λειτουργιών που υποστηρίζονται είναι αρκετά εκτενής και συνεχίζει να αυξάνεται.\nΜπορείτε να βρείτε μία πλήρη λίστα με τις τρέχουσες υποστηριζόμενες συναρτήσεις στο `?acero`.\n\n### Απόδοση {#sec-parquet-fast}\n\nΑς ρίξουμε μία γρήγορη ματιά στο αντίκτυπο της απόδοσης που είχε η μετάβαση από CSV σε parquet.\nΑρχικά, ας ορίσουμε πόσο χρόνο χρειάζεται για να υπολογίσουμε τον αριθμό των βιβλίων που δανείζονται κάθε μήνα του 2021, όταν τα δεδομένα αποθηκεύονται ως ένα μεγάλο csv:\n\n\n::: {.cell hash='arrow_cache/html/dataset-performance-csv_483a703c116b20d0e51a2183c096cfa2'}\n\n```{.r .cell-code}\nseattle_csv |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n#>    user  system elapsed \n#>   8.932   0.775   8.580\n```\n:::\n\n\nΤώρα ας χρησιμοποιήσουμε τη νέα μας έκδοση του συνόλου δεδομένων, στην οποία τα δεδομένα δανεισμού της βιβλιοθήκης του Σιάτλ έχουν χωριστεί σε 18 μικρότερα αρχεία parquet:\n\n\n::: {.cell hash='arrow_cache/html/dataset-performance-multiple-parquet_de9e0ac3cfc08b2e6eef4a12f94f8391'}\n\n```{.r .cell-code}\nseattle_pq |> \n  filter(CheckoutYear == 2021, MaterialType == \"BOOK\") |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n#>    user  system elapsed \n#>   0.196   0.050   0.045\n```\n:::\n\n\nΗ \\~100x επιτάχυνση στην απόδοση αποδίδεται σε δύο παράγοντες: την διαμέριση πολλαπλών αρχείων και τη μορφή των μεμονωμένων αρχείων:\n\n-   Η διαμέριση βελτιώνει την απόδοση επειδή το ερώτημα χρησιμοποιεί το `CheckoutYear == 2021` για να φιλτράρει τα δεδομένα και το arrow είναι αρκετά έξυπνη ώστε να αναγνωρίζει ότι χρειάζεται να διαβάσει μόνο 1 από τα 18 αρχεία parquet.\n-   Η μορφή parquet βελτιώνει την απόδοση, αποθηκεύοντας τα δεδομένα σε δυαδική μορφή, η οποία μπορεί να διαβαστεί πιο άμεσα στη μνήμη. Η κατά στήλη μορφή και τα εμπλουτισμένα μεταδεδομένα σημαίνουν ότι η arrow χρειάζεται μόνο να διαβάσει τις τέσσερις στήλες που χρησιμοποιούνται πραγματικά στο ερώτημα (`CheckoutYear`, `MaterialType`, `CheckoutMonth` και `Checkouts`).\n\nΑυτή η τεράστια διαφορά στην απόδοση είναι η απάντηση στο γιατί συμφέρει να μετατρέψετε μεγάλα CSV σε parquet!\n\n### Συνδυάζοντας τα πακέτα duckdb και arrow\n\nΥπάρχει ένα τελευταίο πλεονέκτημα του parquet και του arrow --- είναι πολύ εύκολο να μετατρέψετε ένα σύνολο δεδομένων arrow σε βάση δεδομένων DuckDB (@sec-import-databases) καλώντας την `arrow::to_duckdb()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_pq |> \n  to_duckdb() |>\n  filter(CheckoutYear >= 2018, MaterialType == \"BOOK\") |>\n  group_by(CheckoutYear) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutYear)) |>\n  collect()\n#> Warning: Missing values are always removed in SQL aggregation functions.\n#> Use `na.rm = TRUE` to silence this warning\n#> This warning is displayed once every 8 hours.\n#> # A tibble: 5 × 2\n#>   CheckoutYear TotalCheckouts\n#>          <int>          <dbl>\n#> 1         2022        2431502\n#> 2         2021        2266438\n#> 3         2020        1241999\n#> 4         2019        3931688\n#> 5         2018        3987569\n```\n:::\n\n\nΤο καλό με την `to_duckdb()` είναι ότι η μεταφορά δεν περιλαμβάνει αντιγραφή στη μνήμη, πράγμα που είναι και στόχος του οικοσυστήματος του arrow: να επιτρέπει συνεχείς μεταβάσεις από το ένα υπολογιστικό περιβάλλον στο άλλο.\n\n### Ασκήσεις\n\n1.  Βρείτε το πιο δημοφιλές βιβλίο κάθε έτος.\n2.  Ποιος συγγραφέας έχει τα περισσότερα βιβλία στο σύστημα της βιβλιοθήκης του Σιάτλ;\n3.  Πώς έχουν αλλάξει οι δανεισμοί των βιβλίων έναντι των ηλεκτρονικών βιβλίων τα τελευταία 10 χρόνια;\n\n## Σύνοψη\n\nΣε αυτό το κεφάλαιο, σας δόθηκε μία γεύση από το πακέτο arrow, το οποίο παρέχει ένα σύστημα υποστήριξης της dplyr για εργασία με μεγάλα σύνολα δεδομένων, αποθηκευμένα στο δίσκο.\nΜπορεί να λειτουργήσει με αρχεία CSV και είναι πολύ πιο γρήγορο αν μετατρέψετε τα δεδομένα σας σε parquet.\nΗ parquet είναι μία δυαδική μορφή δεδομένων που έχει σχεδιαστεί ειδικά για ανάλυση δεδομένων σε σύγχρονους υπολογιστές.\nΠολύ λιγότερα εργαλεία μπορούν να λειτουργήσουν με αρχεία parquet σε σύγκριση με τα CSV, αλλά η διαμερισμένη, συμπιεσμένη και στηλοειδής δομή τους τα καθιστά πολύ πιο αποτελεσματικά στην ανάλυση.\n\nΣτη συνέχεια θα μάθετε για την πρώτη σας πηγή δεδομένων που δεν είναι σε μορφή πίνακα, την οποία θα χειριστείτε χρησιμοποιώντας εργαλεία που παρέχονται από το πακέτο tidyr.\nΘα επικεντρωθούμε σε δεδομένα που προέρχονται από αρχεία JSON, οι γενικές αρχές όμως ισχύουν για δεδομένα που έχουν ιεραρχική (ή δενδροειδή) μορφή ανεξάρτητα από την προέλευσή τους.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}